{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yRIhS6lOTYX",
        "outputId": "0b448614-3adb-494a-ae53-e83c3819339a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCUmGn3I6AJr",
        "outputId": "9d0a0ce8-523a-4436-9a42-a68ba5de070d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjX3Y-kCzdpE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Set a fixed seed value for reproducibility in experiments\n",
        "seed = 123\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHLmmJ1bvaGn"
      },
      "source": [
        "## Build the Model\n",
        "Fine-tuning DistilBERT for QA involves adding a prediction head to output start and end logits for each token:\n",
        "\n",
        "- Start logits predict the start token of the answer span.\n",
        "- End logits predict the end token of the answer span."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7Rtjs5GvYQH"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertModel, DistilBertTokenizerFast, DistilBertConfig\n",
        "import torch.nn as nn\n",
        "\n",
        "class DistilBERTQA(nn.Module):\n",
        "    def __init__(self, model_name='distilbert-base-uncased', num_answer_types=2, dropout_prob=0.1):\n",
        "        super(DistilBERTQA, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained(model_name)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout_seq = nn.Dropout(dropout_prob)  # For sequence output\n",
        "        self.dropout_cls = nn.Dropout(dropout_prob)  # For [CLS] token representation\n",
        "\n",
        "        # Output layers\n",
        "        self.qa_outputs = nn.Linear(self.bert.config.hidden_size, 2)  # Start and end logits\n",
        "        self.answer_type_head = nn.Linear(self.bert.config.hidden_size, num_answer_types)  # Type logits\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        # Apply dropout to sequence output\n",
        "        sequence_output = self.dropout_seq(sequence_output)\n",
        "\n",
        "        # Predict start and end logits\n",
        "        qa_logits = self.qa_outputs(sequence_output)  # Shape: (batch_size, seq_len, 2)\n",
        "        start_logits, end_logits = qa_logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)  # Shape: (batch_size, seq_len)\n",
        "        end_logits = end_logits.squeeze(-1)      # Shape: (batch_size, seq_len)\n",
        "\n",
        "        # Extract [CLS] token representation\n",
        "        cls_representation = sequence_output[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
        "\n",
        "        # Apply dropout to [CLS] representation\n",
        "        cls_representation = self.dropout_cls(cls_representation)\n",
        "\n",
        "        # Predict answer type logits\n",
        "        answer_type_logits = self.answer_type_head(cls_representation)  # Shape: (batch_size, num_answer_types)\n",
        "\n",
        "        return start_logits, end_logits, answer_type_logits\n",
        "\n",
        "\n",
        "# Load Model and Tokenizer\n",
        "def load_model():\n",
        "    model = DistilBERTQA()\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMY11ajAx7Fb"
      },
      "source": [
        "## Dataset and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBrXcF5ZxKMB"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Data instance: {\n",
        "  'name': str,\n",
        "  'id': str,\n",
        "  'questions': [{'input_text': query: str}],\n",
        "  'answers': [\n",
        "         {\n",
        "          'candidate_id': id: int,\n",
        "          'span_text': ans_text: str,\n",
        "          'span_start': start_index: int,\n",
        "          'span_end': end_index: int,\n",
        "          'input_text': type: str ('short' / 'no answer')\n",
        "          }\n",
        "  ],\n",
        "  'has_correct_context': bool,\n",
        "  'contexts': passage: str\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Dataset for QA\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        question = item[\"questions\"][0][\"input_text\"]\n",
        "        context = item[\"contexts\"]\n",
        "        has_answer = item[\"has_correct_context\"]\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            question,\n",
        "            context,\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "        offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "\n",
        "        # Default start and end positions for no-answer ([CLS] token index)\n",
        "        start_positions = 0\n",
        "        end_positions = 0\n",
        "        answer_type = 0  # Default: no answer\n",
        "\n",
        "        if has_answer:\n",
        "            answer = item[\"answers\"][0]\n",
        "            start_char = answer[\"span_start\"]\n",
        "            end_char = answer[\"span_end\"]\n",
        "\n",
        "            for idx, (start, end) in enumerate(offset_mapping):\n",
        "                if start <= start_char < end:\n",
        "                    start_positions = idx\n",
        "                if start < end_char <= end:\n",
        "                    end_positions = idx\n",
        "\n",
        "            answer_type = 1  # Short answer\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(inputs[\"input_ids\"]),\n",
        "            \"attention_mask\": torch.tensor(inputs[\"attention_mask\"]),\n",
        "            \"start_positions\": torch.tensor(start_positions),\n",
        "            \"end_positions\": torch.tensor(end_positions),\n",
        "            \"answer_type\": torch.tensor(answer_type),\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# Load JSON Data\n",
        "def load_data(train_file, val_file):\n",
        "    with open(train_file, \"r\") as f:\n",
        "        train_data = json.load(f)\n",
        "    with open(val_file, \"r\") as f:\n",
        "        val_data = json.load(f)\n",
        "    return train_data, val_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEapIVUnyFNR"
      },
      "source": [
        "## Train Loop and Evaluation Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUEBh4rLyK2s"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Training Loop\n",
        "def train_loop(model, train_data_loader, validation_data_loader, tokenizer, epochs=2, lr=3e-5, weight_decay=0.01, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    A function to train and validate a given model over a specified number of epochs.\n",
        "\n",
        "    Args:\n",
        "        model: The QA model.\n",
        "        train_data_loader: DataLoader for the training dataset.\n",
        "        validation_data_loader: DataLoader for the validation dataset.\n",
        "        tokenizer: Tokenizer used for processing the input text data.\n",
        "        epochs (int): Number of epochs to train the model (default: 2).\n",
        "        lr (float): Learning rate for the optimizer (default: 3e-5).\n",
        "        weight_decay (float): Weight decay parameter for regularization (default: 0.01).\n",
        "        device (str): Device to train on, e.g., 'cpu' or 'cuda' (default: 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        train_losses (list): List of average training losses for each epoch.\n",
        "        val_losses (list): List of validation losses for each epoch.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_data_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\"):\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            start_positions = batch[\"start_positions\"].to(device)\n",
        "            end_positions = batch[\"end_positions\"].to(device)\n",
        "            answer_types = batch[\"answer_type\"].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            start_logits, end_logits, type_logits = model(input_ids, attention_mask)\n",
        "            loss = compute_loss(start_logits, end_logits, type_logits, start_positions, end_positions, answer_types)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        train_losses.append(epoch_loss / len(train_data_loader))\n",
        "\n",
        "        # Validation Loss\n",
        "        val_loss = validate(model, validation_data_loader, tokenizer, device)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_losses[-1]:.4f} | Validation Loss: {val_losses[-1]:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "\n",
        "def validate(model, data_loader, tokenizer, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Validate the model on the validation dataset with batch-wise metrics.\n",
        "\n",
        "    Args:\n",
        "        model: The QA model.\n",
        "        data_loader: DataLoader for the validation dataset.\n",
        "        tokenizer: The tokenizer used to convert input_ids to tokens.\n",
        "        device: Device to run the model on.\n",
        "\n",
        "    Returns:\n",
        "        Average validation loss, batch-wise averaged precision, recall, and F1.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    # To store metrics\n",
        "    batch_precisions, batch_recalls, batch_f1s = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Validating\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            start_positions = batch[\"start_positions\"].to(device)\n",
        "            end_positions = batch[\"end_positions\"].to(device)\n",
        "            answer_types = batch[\"answer_type\"].to(device)\n",
        "\n",
        "            # Get token lists for the batch\n",
        "            batch_token_lists = [tokenizer.convert_ids_to_tokens(ids) for ids in input_ids.cpu().numpy()]\n",
        "\n",
        "            # Forward pass\n",
        "            start_logits, end_logits, type_logits = model(input_ids, attention_mask)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = compute_loss(start_logits, end_logits, type_logits, start_positions, end_positions, answer_types)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            start_preds = torch.argmax(start_logits, dim=-1).cpu().numpy()\n",
        "            end_preds = torch.argmax(end_logits, dim=-1).cpu().numpy()\n",
        "\n",
        "            # Collect predictions and labels\n",
        "            batch_predictions = list(zip(start_preds, end_preds))\n",
        "            batch_labels = list(zip(start_positions.cpu().numpy(), end_positions.cpu().numpy()))\n",
        "\n",
        "            # Compute metrics for the batch\n",
        "            precision, recall, f1 = compute_metrics(batch_predictions, batch_labels, batch_token_lists)\n",
        "            batch_precisions.append(precision)\n",
        "            batch_recalls.append(recall)\n",
        "            batch_f1s.append(f1)\n",
        "\n",
        "    # Average the metrics over all batches\n",
        "    avg_precision = sum(batch_precisions) / len(batch_precisions)\n",
        "    avg_recall = sum(batch_recalls) / len(batch_recalls)\n",
        "    avg_f1 = sum(batch_f1s) / len(batch_f1s)\n",
        "\n",
        "    print(f\"Span Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f}, F1: {avg_f1:.4f}\")\n",
        "\n",
        "    return val_loss / len(data_loader)\n",
        "\n",
        "\n",
        "# Compute Loss\n",
        "def compute_loss(start_logits, end_logits, type_logits, start_positions, end_positions, answer_types):\n",
        "    # Compute start and end loss\n",
        "    start_loss = torch.nn.functional.cross_entropy(start_logits, start_positions)\n",
        "    end_loss = torch.nn.functional.cross_entropy(end_logits, end_positions)\n",
        "\n",
        "    # Compute type loss\n",
        "    type_loss = torch.nn.functional.cross_entropy(type_logits, answer_types)\n",
        "\n",
        "    # Combine the losses\n",
        "    total_loss = start_loss + end_loss + type_loss\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def eval_loop(model, data_loader, tokenizer, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test dataset with batch-wise metrics.\n",
        "\n",
        "    Args:\n",
        "        model: The QA model.\n",
        "        data_loader: DataLoader for the test dataset.\n",
        "        tokenizer: The tokenizer used to convert input_ids to tokens.\n",
        "        device: Device to run the model on.\n",
        "\n",
        "    Returns:\n",
        "        Batch-wise averaged Precision, Recall, and F1 score for spans.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # To store metrics\n",
        "    batch_precisions, batch_recalls, batch_f1s = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            start_positions = batch[\"start_positions\"].to(device)\n",
        "            end_positions = batch[\"end_positions\"].to(device)\n",
        "\n",
        "            # Get token lists for the batch\n",
        "            batch_token_lists = [tokenizer.convert_ids_to_tokens(ids) for ids in input_ids.cpu().numpy()]\n",
        "\n",
        "            # Forward pass\n",
        "            start_logits, end_logits, type_logits = model(input_ids, attention_mask)\n",
        "\n",
        "            # Get predictions\n",
        "            start_preds = torch.argmax(start_logits, dim=-1).cpu().numpy()\n",
        "            end_preds = torch.argmax(end_logits, dim=-1).cpu().numpy()\n",
        "\n",
        "            # Collect predictions and labels\n",
        "            batch_predictions = list(zip(start_preds, end_preds))\n",
        "            batch_labels = list(zip(start_positions.cpu().numpy(), end_positions.cpu().numpy()))\n",
        "\n",
        "            # Compute metrics for the batch\n",
        "            precision, recall, f1 = compute_metrics(batch_predictions, batch_labels, batch_token_lists)\n",
        "            batch_precisions.append(precision)\n",
        "            batch_recalls.append(recall)\n",
        "            batch_f1s.append(f1)\n",
        "\n",
        "    # Average the metrics over all batches\n",
        "    avg_precision = sum(batch_precisions) / len(batch_precisions)\n",
        "    avg_recall = sum(batch_recalls) / len(batch_recalls)\n",
        "    avg_f1 = sum(batch_f1s) / len(batch_f1s)\n",
        "\n",
        "    return avg_precision, avg_recall, avg_f1\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(predictions, labels, tokenized_inputs):\n",
        "    \"\"\"\n",
        "    Compute precision, recall, and F1 score based on token overlaps.\n",
        "\n",
        "    Args:\n",
        "        predictions: List of tuples [(start_pred, end_pred), ...] for each example.\n",
        "        labels: List of tuples [(start_label, end_label), ...] for each example.\n",
        "        tokenized_inputs: List of token lists corresponding to each example.\n",
        "\n",
        "    Returns:\n",
        "        precision, recall, f1\n",
        "    \"\"\"\n",
        "    total_tp = 0\n",
        "    total_fp = 0\n",
        "    total_fn = 0\n",
        "\n",
        "    for (pred, label, tokens) in zip(predictions, labels, tokenized_inputs):\n",
        "        (s_pred, e_pred) = pred\n",
        "        (s_label, e_label) = label\n",
        "\n",
        "        # Ensure start <= end\n",
        "        if s_pred > e_pred:\n",
        "            predicted_tokens = set()\n",
        "        else:\n",
        "            predicted_tokens = set(tokens[s_pred : e_pred + 1])\n",
        "\n",
        "        if s_label > e_label:\n",
        "            gold_tokens = set()\n",
        "        else:\n",
        "            gold_tokens = set(tokens[s_label : e_label + 1])\n",
        "\n",
        "        # Calculate true positives, false positives, and false negatives\n",
        "        tp = len(predicted_tokens & gold_tokens)\n",
        "        fp = len(predicted_tokens - gold_tokens)\n",
        "        fn = len(gold_tokens - predicted_tokens)\n",
        "\n",
        "        total_tp += tp\n",
        "        total_fp += fp\n",
        "        total_fn += fn\n",
        "\n",
        "    # Compute precision, recall, f1\n",
        "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
        "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
        "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZtOLnqBS7E7",
        "outputId": "e6d014c1-e461-4656-a7aa-0536387d944d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/2: 100%|██████████| 871/871 [22:40<00:00,  1.56s/it]\n",
            "Validating: 100%|██████████| 1743/1743 [00:30<00:00, 57.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Span Precision: 0.6530, Recall: 0.7101, F1: 0.6509\n",
            "Epoch 1/2 | Train Loss: 3.7811 | Validation Loss: 2.7316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/2: 100%|██████████| 871/871 [22:40<00:00,  1.56s/it]\n",
            "Validating: 100%|██████████| 1743/1743 [00:30<00:00, 57.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Span Precision: 0.7111, Recall: 0.7312, F1: 0.6965\n",
            "Epoch 2/2 | Train Loss: 2.2436 | Validation Loss: 2.4962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 1743/1743 [00:30<00:00, 57.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "PRECISION:  0.711126624192353\n",
            "RECALL:  0.7311867682020301\n",
            "F1-SCORE:  0.6965455687260241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Main Function\n",
        "def main():\n",
        "    DIR_PATH = \"/content/drive/MyDrive/Final Project/Data/\"\n",
        "    train_file = DIR_PATH + \"all_train.json\"\n",
        "    val_file = DIR_PATH + \"all_dev.json\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    batch_size = 32\n",
        "\n",
        "    model, tokenizer = load_model()\n",
        "    train_data, validation_data = load_data(train_file, val_file)\n",
        "\n",
        "    train_data_loader = DataLoader(QADataset(train_data, tokenizer), batch_size=batch_size, shuffle=True)\n",
        "    validation_data_loader = DataLoader(QADataset(validation_data, tokenizer), batch_size=1)\n",
        "\n",
        "    train_losses, val_losses = train_loop(model, train_data_loader, validation_data_loader, tokenizer, device=device)\n",
        "    precision, recall, f1_score = eval_loop(model, validation_data_loader, tokenizer, device=device)\n",
        "\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"PRECISION: \", precision)\n",
        "    print(\"RECALL: \", recall)\n",
        "    print(\"F1-SCORE: \", f1_score)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}